name: open-hallucination-api

services:
  # 1. Knowledge Graph (Neo4j)
  neo4j:
    env_file:
      - ../../.env
    image: neo4j:latest
    container_name: ohi-neo4j
    ports:
      - "127.0.0.1:7474:7474" # HTTP - localhost only for dev
      - "127.0.0.1:7687:7687" # Bolt - localhost only for dev
    environment:
      - NEO4J_AUTH=neo4j/password123
      - NEO4J_PLUGINS=["apoc", "graph-data-science", "bloom"]
      - NEO4J_server_memory_heap_initial__size=4G
      - NEO4J_server_memory_heap_max__size=16G  
      - NEO4J_server_memory_pagecache_size=8G  
    volumes:
      - ../data/data/neo4j/data:/data
      - ../data/data/neo4j/logs:/logs
    networks:
      - ohi-net

  # 2. Vector Database (Qdrant)
  qdrant:
    env_file:
      - ../../.env
    image: qdrant/qdrant:latest
    container_name: ohi-qdrant
    user: root
    ports:
      - "127.0.0.1:6333:6333" # localhost only for dev
      - "127.0.0.1:6334:6334" # gRPC for ingestion
    volumes:
      - ../data/data/qdrant:/qdrant/storage
    networks:
      - ohi-net

  # 3. Cache (Redis)
  redis:
    env_file:
      - ../../.env
    image: redis:latest
    container_name: ohi-redis
    ports:
      - "127.0.0.1:6379:6379" # localhost only for dev
    volumes:
      - ../data/data/redis:/data
    networks:
      - ohi-net

  # 4. vLLM (GPU Accelerated) - mistralai/Mistral-7B-Instruct-v0.2
  vllm:
    env_file:
      - ../../.env
    image: vllm/vllm-openai:latest
    container_name: ohi-vllm
    ports:
      - "127.0.0.1:8000:8000" # localhost only for dev
    # Shared Memory ist kritisch fÃ¼r PyTorch/NCCL
    ipc: host
    # Note: UDP buffer sysctls (net.core.rmem_max/wmem_max) are not supported 
    # on Docker Desktop for Windows. Set these on the host if needed:
    # sysctl -w net.core.rmem_max=7500000 net.core.wmem_max=7500000
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - TORCHINDUCTOR_DISABLE=1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - LD_LIBRARY_PATH=/usr/lib/wsl/lib:${LD_LIBRARY_PATH}
      - VLLM_LOGGING_LEVEL=WARNING
    runtime: nvidia
    gpus: all
    # NVIDIA GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      # Model-Cache persistent machen
      - ~/.cache/huggingface:/root/.cache/huggingface
      # WSL2 NVIDIA driver libs (provides libcuda.so)
      - /usr/lib/wsl:/usr/lib/wsl:ro
    # TheBloke/openinstruct-mistral-7B-AWQ
    # Optimized for RTX 4090 (24GB VRAM)
    command: >
      --model TheBloke/openinstruct-mistral-7B-AWQ
      --dtype half
      --max-model-len 4096
      --max-num-seqs 64
      --gpu-memory-utilization 0.7
      --tensor-parallel-size 1
      --enable-prefix-caching
      --enable-chunked-prefill
      --disable-log-requests
    networks:
      - ohi-net

  # 5. OHI MCP Server - Unified Knowledge Sources
  ohi-mcp-server:
    env_file:
      - ../../.env
    build:
      context: ../..
      dockerfile: docker/mcp-server/Dockerfile
    image: ohi-mcp-server:local
    container_name: ohi-mcp-server
    ports:
      - "127.0.0.1:8083:8080" # localhost only for dev
    environment:
      - NODE_ENV=production
      - LOG_LEVEL=error
      - CONTEXT7_API_KEY=${CONTEXT7_API_KEY}
      - CONTEXT7_BASE_URL=${CONTEXT7_BASE_URL:-https://context7.com}
      - MCP_MAX_PARALLEL_SOURCES=1
      - MCP_SOURCE_DELAY_MS=250
      # Polite pool email for Crossref & OpenAlex (higher rate limits)
      # Crossref: 50 req/sec (vs 1 req/sec anonymous)
      # OpenAlex: 100k req/day with faster response (vs rate-limited anonymous)
      - POLITE_POOL_EMAIL=${POLITE_POOL_EMAIL:-hi@shiftbloom.studio}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - ohi-net

  # 7. Open Hallucination Index API
  ohi-api:
    env_file:
      - ../../.env
    build:
      context: ../..
      dockerfile: docker/api/Dockerfile
    image: open-hallucination-index:local
    container_name: ohi-api
    # Internal only - nginx handles external traffic
    expose:
      - "8080"
    environment:
      API_HOST: "0.0.0.0"
      API_PORT: "8080"
      API_WORKERS: "1"
      API_API_KEY: "${API_API_KEY}"

      # Use in-network service names
      LLM_BASE_URL: "http://vllm:8000/v1"
      LLM_MODEL: "TheBloke/openinstruct-mistral-7B-AWQ"
      LLM_API_KEY: "${LLM_API_KEY:-no-key-required}"
      
      # Local embeddings (no OpenAI API needed)
      EMBEDDING_MODEL_NAME: "all-MiniLM-L12-v2"
      EMBEDDING_BATCH_SIZE: "32"

      NEO4J_URI: "bolt://neo4j:7687"
      NEO4J_USERNAME: "neo4j"
      NEO4J_PASSWORD: "password123"

      # Qdrant with local embedding size (384 for all-MiniLM-L12-v2)
      QDRANT_HOST: "qdrant"
      # QDRANT_COLLECTION_NAME: "wikipedia_hybrid"  # Disabled - data was lost, using default
      QDRANT_VECTOR_SIZE: "384"
      REDIS_HOST: "redis"

      # MCP Server configuration
      MCP_WIKIPEDIA_ENABLED: "true"
      MCP_WIKIPEDIA_URL: "http://ohi-mcp-server:8080"
      MCP_CONTEXT7_ENABLED: "true"
      MCP_CONTEXT7_URL: "http://ohi-mcp-server:8080"
      CONTEXT7_API_KEY: "${CONTEXT7_API_KEY}"
      MCP_OHI_ENABLED: "true"
      MCP_OHI_URL: "http://ohi-mcp-server:8080"

      # Verification settings
      VERIFY_DEFAULT_STRATEGY: "mcp_enhanced"
      VERIFY_PERSIST_MCP_EVIDENCE: "true"
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"

    depends_on:
      - neo4j
      - qdrant
      - redis
      - vllm
      - ohi-mcp-server
    runtime: nvidia
    gpus: all
    networks:
      - ohi-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    volumes:
      - redis-socket:/run/redis
    restart: unless-stopped

  # 7b. Benchmark Runner (docker exec friendly)
  benchmark-runner:
    env_file:
      - ../../.env
    image: open-hallucination-index:local
    container_name: ohi-benchmark
    entrypoint: ["sleep", "infinity"]
    environment:
      OHI_API_HOST: "ohi-api"
      OHI_API_PORT: "8080"
      API_API_KEY: "${API_API_KEY}"
      BENCHMARK_OUTPUT_DIR: "/app/benchmark_results"
      BENCHMARK_NO_PROGRESS: "true"
    depends_on:
      - ohi-api
    networks:
      - ohi-net
    volumes:
      - ../../benchmark_results:/app/benchmark_results

  # 8. Nginx Reverse Proxy with SSL
  nginx:
    env_file:
      - ../../.env
    image: nginx:latest
    container_name: ohi-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ../nginx/nginx.conf:/etc/nginx/conf.d/default.conf:ro
      - ../data/data/certbot/conf:/etc/letsencrypt:ro
      - ../data/data/certbot/www:/var/www/certbot:ro
    depends_on:
      - ohi-api
      - ohi-frontend
    networks:
      - ohi-net
    command: "/bin/sh -c 'while :; do sleep 6h & wait $${!}; nginx -s reload; done & nginx -g \"daemon off;\"'"
    restart: unless-stopped

  # 8b. Cloudflare Tunnel (bypasses IPv6/Docker issues)
  cloudflared:
    env_file:
      - ../../.env
    image: cloudflare/cloudflared:latest
    container_name: ohi-cloudflared
    command: tunnel run
    environment:
      - TUNNEL_TOKEN=${CLOUDFLARE_TUNNEL_TOKEN}
    depends_on:
      - nginx
    networks:
      - ohi-net
    restart: unless-stopped

  # 9. Frontend (Next.js App)
  ohi-frontend:
    env_file:
      - ../../.env
    build:
      context: ../../src/frontend
      dockerfile: Dockerfile
      args:
        - NEXT_PUBLIC_SUPABASE_URL=${NEXT_PUBLIC_SUPABASE_URL:-}
        - NEXT_PUBLIC_SUPABASE_ANON_KEY=${NEXT_PUBLIC_SUPABASE_ANON_KEY:-}
        - NEXT_PUBLIC_APP_URL=${NEXT_PUBLIC_APP_URL:-}
        - DEFAULT_API_URL=${DEFAULT_API_URL:-}
        - DEFAULT_API_KEY=${DEFAULT_API_KEY:-}
    image: ohi-frontend:local
    container_name: ohi-frontend
    expose:
      - "3000"
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_SUPABASE_URL=${NEXT_PUBLIC_SUPABASE_URL:-}
      - NEXT_PUBLIC_SUPABASE_ANON_KEY=${NEXT_PUBLIC_SUPABASE_ANON_KEY:-}
      - SUPABASE_SERVICE_ROLE_KEY=${SUPABASE_SERVICE_ROLE_KEY:-}
      - NEXT_PUBLIC_APP_URL=${NEXT_PUBLIC_APP_URL:-}
      - DATABASE_URL=${DATABASE_URL:-}
      - DEFAULT_API_URL=${DEFAULT_API_URL:-}
      - DEFAULT_API_KEY=${DEFAULT_API_KEY:-}
      - STRIPE_SECRET_KEY=${STRIPE_SECRET_KEY:-}
      - STRIPE_WEBHOOK_SECRET=${STRIPE_WEBHOOK_SECRET:-}
    depends_on:
      - ohi-api
    networks:
      - ohi-net
    restart: unless-stopped

networks:
  ohi-net:
    driver: bridge

volumes:
  redis-socket: