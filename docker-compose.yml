services:
  # 1. Knowledge Graph (Neo4j)
  neo4j:
    image: neo4j:2025.11.2
    container_name: ohi-neo4j
    ports:
      - "7474:7474" # HTTP
      - "7687:7687" # Bolt
    environment:
      - NEO4J_AUTH=neo4j/password123
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      # Performance Tuning
      - NEO4J_dbms_memory_heap_initial__size=1G
      - NEO4J_dbms_memory_heap_max__size=2G
    volumes:
      - ./data/neo4j/data:/data
      - ./data/neo4j/logs:/logs
    networks:
      - ohi-net

  # 2. Vector Database (Qdrant)
  qdrant:
    image: qdrant/qdrant:v1.16.3
    container_name: ohi-qdrant
    ports:
      - "6333:6333"
    volumes:
      - ./data/qdrant:/qdrant/storage
    networks:
      - ohi-net

  # 3. Cache (Redis)
  redis:
    image: redis:8.4.0
    container_name: ohi-redis
    ports:
      - "6379:6379"
    volumes:
      - ./data/redis:/data
    networks:
      - ohi-net

  # 4. vLLM (GPU Accelerated) - Upgraded to Qwen2.5-32B with CPU offload
  vllm:
    image: vllm/vllm-openai:latest
    container_name: ohi-vllm
    ports:
      - "8000:8000"
    # Shared Memory ist kritisch fÃ¼r PyTorch/NCCL
    ipc: host
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
    # NVIDIA GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      # Model-Cache persistent machen
      - ~/.cache/huggingface:/root/.cache/huggingface
    # Qwen2.5-7B with AWQ quantization for 24GB VRAM
    command: >
      --model Qwen/Qwen2.5-7B-Instruct-AWQ
      --quantization awq
      --dtype half
      --max-model-len 8192
      --max-num-seqs 16
      --enforce-eager
      --gpu-memory-utilization 0.90
      --tensor-parallel-size 1
    networks:
      - ohi-net

  # 5. Wikipedia MCP Server (Sidecar)
  wikipedia-mcp:
    build:
      context: ./docker/wikipedia-mcp
      dockerfile: Dockerfile
    container_name: ohi-wikipedia-mcp
    ports:
      - "8081:8080"
    restart: unless-stopped
    networks:
      - ohi-net

  # 6. Context7 MCP Server (Sidecar)
  context7-mcp:
    build:
      context: ./docker/context7-mcp
      dockerfile: Dockerfile
    container_name: ohi-context7-mcp
    ports:
      - "8082:3000"
    environment:
      - CONTEXT7_API_KEY=${CONTEXT7_API_KEY}
    restart: unless-stopped
    networks:
      - ohi-net

  # 7. Open Hallucination Index API
  ohi-api:
    build:
      context: .
      dockerfile: Dockerfile
    image: open-hallucination-index:local
    container_name: ohi-api
    # Internal only - nginx handles external traffic
    expose:
      - "8080"
    environment:
      API_HOST: "0.0.0.0"
      API_PORT: "8080"
      API_WORKERS: "4"
      API_API_KEY: "${API_KEY}"

      # Use in-network service names
      LLM_BASE_URL: "http://vllm:8000/v1"
      LLM_MODEL: "Qwen/Qwen2.5-7B-Instruct-AWQ"
      LLM_API_KEY: "${LLM_API_KEY:-no-key-required}"
      
      # Local embeddings (no OpenAI API needed)
      EMBEDDING_MODEL_NAME: "all-MiniLM-L6-v2"
      EMBEDDING_BATCH_SIZE: "32"

      NEO4J_URI: "bolt://neo4j:7687"
      NEO4J_USERNAME: "neo4j"
      NEO4J_PASSWORD: "password123"

      # Qdrant with local embedding size (384 for all-MiniLM-L6-v2)
      QDRANT_HOST: "qdrant"
      QDRANT_VECTOR_SIZE: "384"
      REDIS_HOST: "redis"

      # MCP Server configuration
      MCP_WIKIPEDIA_ENABLED: "true"
      MCP_WIKIPEDIA_URL: "http://wikipedia-mcp:8080"
      MCP_CONTEXT7_ENABLED: "true"
      MCP_CONTEXT7_URL: "http://context7-mcp:3000"
      CONTEXT7_API_KEY: "${CONTEXT7_API_KEY}"

      # Verification settings
      VERIFY_DEFAULT_STRATEGY: "mcp_enhanced"
      VERIFY_PERSIST_MCP_EVIDENCE: "true"

    depends_on:
      - neo4j
      - qdrant
      - redis
      - vllm
      - wikipedia-mcp
      - context7-mcp
    networks:
      - ohi-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    volumes:
      - redis-socket:/run/redis
    restart: unless-stopped

  # 8. Nginx Reverse Proxy with SSL
  nginx:
    image: nginx:1.27-alpine
    container_name: ohi-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./docker/nginx/nginx.conf:/etc/nginx/conf.d/default.conf:ro
      - ./data/certbot/conf:/etc/letsencrypt:ro
      - ./data/certbot/www:/var/www/certbot:ro
    depends_on:
      - ohi-api
    networks:
      - ohi-net
    command: "/bin/sh -c 'while :; do sleep 6h & wait $${!}; nginx -s reload; done & nginx -g \"daemon off;\"'"
    restart: unless-stopped

  # 9. Certbot for Let's Encrypt certificates
  certbot:
    image: certbot/certbot:v3.3.0
    container_name: ohi-certbot
    volumes:
      - ./data/certbot/conf:/etc/letsencrypt
      - ./data/certbot/www:/var/www/certbot
    networks:
      - ohi-net
    entrypoint: "/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done;'"

networks:
  ohi-net:
    driver: bridge

volumes:
  redis-socket: