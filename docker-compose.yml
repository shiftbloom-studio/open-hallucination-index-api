services:
  # 1. Knowledge Graph (Neo4j)
  neo4j:
    image: neo4j:2025.11.2
    container_name: ohi-neo4j
    ports:
      - "127.0.0.1:7474:7474" # HTTP - localhost only for dev
      - "127.0.0.1:7687:7687" # Bolt - localhost only for dev
    environment:
      - NEO4J_AUTH=neo4j/password123
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      - NEO4J_server_memory_heap_initial__size=4G
      - NEO4J_server_memory_heap_max__size=16G  
      - NEO4J_server_memory_pagecache_size=8G  
    volumes:
      - ./data/neo4j/data:/data
      - ./data/neo4j/logs:/logs
    networks:
      - ohi-net

  # 2. Vector Database (Qdrant)
  qdrant:
    image: qdrant/qdrant:v1.16.3
    container_name: ohi-qdrant
    ports:
      - "127.0.0.1:6333:6333" # localhost only for dev
    volumes:
      - ./data/qdrant:/qdrant/storage
    networks:
      - ohi-net

  # 3. Cache (Redis)
  redis:
    image: redis:8.4.0
    container_name: ohi-redis
    ports:
      - "127.0.0.1:6379:6379" # localhost only for dev
    volumes:
      - ./data/redis:/data
    networks:
      - ohi-net

  # 4. vLLM (GPU Accelerated) - Qwen2.5-14B full precision
  vllm:
    image: vllm/vllm-openai:latest
    container_name: ohi-vllm
    ports:
      - "127.0.0.1:8000:8000" # localhost only for dev
    # Shared Memory ist kritisch fÃ¼r PyTorch/NCCL
    ipc: host
    # Note: UDP buffer sysctls (net.core.rmem_max/wmem_max) are not supported 
    # on Docker Desktop for Windows. Set these on the host if needed:
    # sysctl -w net.core.rmem_max=7500000 net.core.wmem_max=7500000
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
    # NVIDIA GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      # Model-Cache persistent machen
      - ~/.cache/huggingface:/root/.cache/huggingface
    # Qwen2.5-14B with AWQ 4-bit quantization (fits in 24GB GPU)
    # Using --generation-config vllm to use model's recommended sampling params
    # (temp=0.7, top_p=0.8, top_k=20, repetition_penalty=1.05)
    command: >
      --model Qwen/Qwen2.5-14B-Instruct-AWQ
      --quantization awq
      --dtype half
      --max-model-len 8192
      --max-num-seqs 16
      --gpu-memory-utilization 0.90
      --tensor-parallel-size 1
      --generation-config vllm
    networks:
      - ohi-net

  # 5. Wikipedia MCP Server (Sidecar)
  wikipedia-mcp:
    build:
      context: ./docker/wikipedia-mcp
      dockerfile: Dockerfile
    container_name: ohi-wikipedia-mcp
    ports:
      - "127.0.0.1:8081:8080" # localhost only for dev
    environment:
      - PYTHONUNBUFFERED=1
      - MCP_LOG_LEVEL=WARNING
    restart: unless-stopped
    networks:
      - ohi-net

  # 6. Context7 MCP Server (Sidecar)
  context7-mcp:
    build:
      context: ./docker/context7-mcp
      dockerfile: Dockerfile
    container_name: ohi-context7-mcp
    ports:
      - "127.0.0.1:8082:3000" # localhost only for dev
    environment:
      - CONTEXT7_API_KEY=${CONTEXT7_API_KEY}
    restart: unless-stopped
    networks:
      - ohi-net

  # 6b. OHI MCP Server - Unified Knowledge Sources
  ohi-mcp-server:
    build:
      context: ./docker/ohi-mcp-server
      dockerfile: Dockerfile
      cache_from:
        - ohi-mcp-server:local
    image: ohi-mcp-server:local
    container_name: ohi-mcp-server
    ports:
      - "127.0.0.1:8083:8080" # localhost only for dev
    environment:
      - NODE_ENV=production
      - LOG_LEVEL=info
      # Polite pool email for Crossref & OpenAlex (higher rate limits)
      # Crossref: 50 req/sec (vs 1 req/sec anonymous)
      # OpenAlex: 100k req/day with faster response (vs rate-limited anonymous)
      - POLITE_POOL_EMAIL=${POLITE_POOL_EMAIL:-hi@shiftbloom.studio}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - ohi-net

  # 7. Open Hallucination Index API
  ohi-api:
    build:
      context: .
      dockerfile: Dockerfile
      cache_from:
        - open-hallucination-index:local
    image: open-hallucination-index:local
    container_name: ohi-api
    # Internal only - nginx handles external traffic
    expose:
      - "8080"
    environment:
      API_HOST: "0.0.0.0"
      API_PORT: "8080"
      API_WORKERS: "4"
      API_API_KEY: "${API_API_KEY}"

      # Use in-network service names
      LLM_BASE_URL: "http://vllm:8000/v1"
      LLM_MODEL: "Qwen/Qwen2.5-14B-Instruct-AWQ"
      LLM_API_KEY: "${LLM_API_KEY:-no-key-required}"
      
      # Local embeddings (no OpenAI API needed)
      EMBEDDING_MODEL_NAME: "all-MiniLM-L6-v2"
      EMBEDDING_BATCH_SIZE: "32"

      NEO4J_URI: "bolt://neo4j:7687"
      NEO4J_USERNAME: "neo4j"
      NEO4J_PASSWORD: "password123"

      # Qdrant with local embedding size (384 for all-MiniLM-L6-v2)
      QDRANT_HOST: "qdrant"
      QDRANT_VECTOR_SIZE: "384"
      REDIS_HOST: "redis"

      # MCP Server configuration
      MCP_WIKIPEDIA_ENABLED: "true"
      MCP_WIKIPEDIA_URL: "http://wikipedia-mcp:8080"
      MCP_CONTEXT7_ENABLED: "true"
      MCP_CONTEXT7_URL: "http://context7-mcp:3000"
      CONTEXT7_API_KEY: "${CONTEXT7_API_KEY}"
      MCP_OHI_ENABLED: "true"
      MCP_OHI_URL: "http://ohi-mcp-server:8080"

      # Verification settings
      VERIFY_DEFAULT_STRATEGY: "mcp_enhanced"
      VERIFY_PERSIST_MCP_EVIDENCE: "true"

    depends_on:
      - neo4j
      - qdrant
      - redis
      - vllm
      - wikipedia-mcp
      - context7-mcp
      - ohi-mcp-server
    networks:
      - ohi-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    volumes:
      - redis-socket:/run/redis
    restart: unless-stopped

  # 8. Nginx Reverse Proxy with SSL
  nginx:
    image: nginx:1.27-alpine
    container_name: ohi-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./docker/nginx/nginx.conf:/etc/nginx/conf.d/default.conf:ro
      - ./data/certbot/conf:/etc/letsencrypt:ro
      - ./data/certbot/www:/var/www/certbot:ro
    depends_on:
      - ohi-api
      - ohi-frontend
    networks:
      - ohi-net
    command: "/bin/sh -c 'while :; do sleep 6h & wait $${!}; nginx -s reload; done & nginx -g \"daemon off;\"'"
    restart: unless-stopped

  # 8b. Cloudflare Tunnel (bypasses IPv6/Docker issues)
  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: ohi-cloudflared
    command: tunnel run
    environment:
      - TUNNEL_TOKEN=${CLOUDFLARE_TUNNEL_TOKEN}
    depends_on:
      - nginx
    networks:
      - ohi-net
    restart: unless-stopped

  # 9. Certbot for Let's Encrypt certificates
  certbot:
    image: certbot/certbot:v5.2.2
    container_name: ohi-certbot
    volumes:
      - ./data/certbot/conf:/etc/letsencrypt
      - ./data/certbot/www:/var/www/certbot
    networks:
      - ohi-net
    entrypoint: "/bin/sh -c 'tail -f /dev/null'"

  # 10. Frontend (Next.js App)
  ohi-frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      cache_from:
        - ohi-frontend:local
      args:
        - NEXT_PUBLIC_SUPABASE_URL=${NEXT_PUBLIC_SUPABASE_URL:-}
        - NEXT_PUBLIC_SUPABASE_ANON_KEY=${NEXT_PUBLIC_SUPABASE_ANON_KEY:-}
        - NEXT_PUBLIC_APP_URL=${NEXT_PUBLIC_APP_URL:-}
        - DEFAULT_API_URL=${DEFAULT_API_URL:-}
        - DEFAULT_API_KEY=${DEFAULT_API_KEY:-}
    image: ohi-frontend:local
    container_name: ohi-frontend
    expose:
      - "3000"
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_SUPABASE_URL=${NEXT_PUBLIC_SUPABASE_URL:-}
      - NEXT_PUBLIC_SUPABASE_ANON_KEY=${NEXT_PUBLIC_SUPABASE_ANON_KEY:-}
      - SUPABASE_SERVICE_ROLE_KEY=${SUPABASE_SERVICE_ROLE_KEY:-}
      - NEXT_PUBLIC_APP_URL=${NEXT_PUBLIC_APP_URL:-}
      - DATABASE_URL=${DATABASE_URL:-}
      - DEFAULT_API_URL=${DEFAULT_API_URL:-}
      - DEFAULT_API_KEY=${DEFAULT_API_KEY:-}
      - STRIPE_SECRET_KEY=${STRIPE_SECRET_KEY:-}
      - STRIPE_WEBHOOK_SECRET=${STRIPE_WEBHOOK_SECRET:-}
    depends_on:
      - ohi-api
    networks:
      - ohi-net
    restart: unless-stopped

networks:
  ohi-net:
    driver: bridge

volumes:
  redis-socket: